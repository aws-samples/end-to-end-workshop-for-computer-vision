{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Models for Inference\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Deploy pretrained model to SageMaker Endpoint](#Deploy-pretrained-model-to-SageMaker-Endpoint)\n",
    "    1. [Model Config](#Model-Config)\n",
    "    2. [Option 1: Serverless Inference](#Option-1:-Serverless-Inference)\n",
    "        1. [Serverless Inference Deploy Config](#Serverless-Inference-Deploy-Config)\n",
    "        2. [Serverless Inference Deployment](#Serverless-Inference-Deployment)\n",
    "    3. [Option 2: Real-time Inference](#Option-2:-Real-time-Inference)\n",
    "        1. [Real-time Inference Deployment](#Real-time-Inference-Deployment)\n",
    "5. [Run Inference on Deployed Endpoint](#Run-Inference-on-Deployed-Endpoint)\n",
    "    1. [Create Predictor from Inference Endpoint](#Create-Predictor-from-Inference-Endpoint)\n",
    "    2. [Get trained Classes Info](#Get-trained-Classes-Info)\n",
    "    3. [Download sample test images from S3 for inference](#Download-sample-test-images-from-S3-for-inference)\n",
    "    4. [Get Predictions from local images](#Get-Predictions-from-local-images)\n",
    "6. [Clean Up](#Clean-Up)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "After you build and train your models, you can deploy them to get predictions. \n",
    "\n",
    "SageMaker supports multiple deployment types for customers to choose from, based on the requirements, like Real-time inference, Serverless inference, Asynchronous inference, and Batch transform. \n",
    "\n",
    "To learn more about deploying models for inference using SageMaker refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).  \n",
    "\n",
    "** Note: This Notebook was tested on Data Science Kernel for SageMaker Studio**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\r\n",
    "\r\n",
    "To run this notebook, you can simply execute each cell in order. To understand what's happening, you'll need:\r\n",
    "\r\n",
    "- Prepare our model for deployment (use the model from the previous modules)\r\n",
    "- Familiarity with Python and numpy\r\n",
    "- Basic familiarity with AWS S3.\r\n",
    "- Basic understanding of AWS Sagemaker.\r\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\r\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Setting up the environment, load the libraries, and define the parameters for the entire notebook.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "import boto3\r\n",
    "import botocore\r\n",
    "import json\r\n",
    "\r\n",
    "role = get_execution_role()\r\n",
    "sess = sagemaker.Session()\r\n",
    "region = boto3.Session().region_name\r\n",
    "\r\n",
    "# Feel free to specify different bucket/folders here if you wish.\r\n",
    "bucket = sess.default_bucket()\r\n",
    "prefix = 'cv-sagemaker-immersionday'\r\n",
    "\r\n",
    "\r\n",
    "TF_FRAMEWORK_VERSION = '2.4.1'\r\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.c5.4xlarge'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy pretrained model to SageMaker Endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Config\n",
    "\n",
    "You can use the bird model created from the previous modules (ref. [02_training/training.ipynb](../02_training/training.ipynb)), by executing the cell below.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get S3 URI of the model artifact\r\n",
    "model_dir = f's3://{bucket}/{prefix}/outputs/model'\r\n",
    "\r\n",
    "bird_model_path = f'{model_dir}/model.tar.gz'\r\n",
    "print(f'bird_model_path: {bird_model_path}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choose the deployment option for Inference\n",
    "\n",
    "Run either Option 1 or Option 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 1: Serverless Inference\n",
    "\n",
    "Amazon SageMaker Serverless Inference is a new inference option that enables you to easily deploy machine learning models for inference without having to configure or manage the underlying infrastructure. \n",
    "\n",
    "Simply select the serverless option when deploying your machine learning model, and Amazon SageMaker automatically provisions, scales, and turns off compute capacity based on the volume of inference requests. \n",
    "\n",
    "With SageMaker Serverless Inference, you pay only for the duration of running the inference code and the amount of data processed, not for idle time. For more information on how Serverless Inference works visit [here](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Serverless Inference Deploy Config\n",
    "\n",
    "This object specifies configuration related to serverless endpoint. Use this configuration when trying to create serverless endpoint and make serverless inference\n",
    "\n",
    "Initialize a ServerlessInferenceConfig object for serverless inference configuration.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker.serverless as Serverless\r\n",
    "\r\n",
    "serverless_inf_config = Serverless.ServerlessInferenceConfig(\r\n",
    "    memory_size_in_mb=4096, \r\n",
    "    max_concurrency=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Serverless Inference Deployment\n",
    "\n",
    "Deploy the Model to a serverless endpoint and return a Predictor object to make serverless inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\r\n",
    "\r\n",
    "model = TensorFlowModel(\r\n",
    "    model_data=bird_model_path, \r\n",
    "    role=role,\r\n",
    "    framework_version=TF_FRAMEWORK_VERSION)\r\n",
    "\r\n",
    "\r\n",
    "predictor = model.deploy(serverless_inference_config=serverless_inf_config)\r\n",
    "tf_endpoint_name = str(predictor.endpoint_name)\r\n",
    "print(f\"Endpoint [{predictor.endpoint_name}] deployed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The endpoint name will be displayed in the previous cell output when it's active and can also be seen under the SageMaker Resources option which is on the left side bar of the Studio as well. You will need it in the next section to create predictor from Inference endpoint\n",
    "\n",
    "![Active Endpoint](statics/active-sagemaker-endpoints.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2: Real-time Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Real-time Inference Deployment\n",
    "\n",
    "Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.\n",
    "\n",
    "Deploy the Model to a real-time endpoint and return a Predictor object to make inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\r\n",
    "\r\n",
    "model = TensorFlowModel(\r\n",
    "    model_data=bird_model_path,\r\n",
    "    role=role,\r\n",
    "    framework_version=TF_FRAMEWORK_VERSION)\r\n",
    "\r\n",
    "\r\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=ENDPOINT_INSTANCE_TYPE)\r\n",
    "tf_endpoint_name = str(predictor.endpoint_name)\r\n",
    "print(f\"Endpoint [{predictor.endpoint_name}] deployed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The endpoint name will be displayed in the previous cell output when it's active and can also be seen under the SageMaker Resources option which is on the left side bar of the Studio as well. You will need it in the next section to create predictor from Inference endpoint\n",
    "\n",
    "![Active Endpoint](statics/active-sagemaker-endpoints.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Inference on Deployed Endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Predictor from Inference Endpoint\n",
    "\n",
    "After the deployment is complete in the above step, capture the endpoint name from SageMaker console and input below in the Predictor config. We could have reused the predictor from above step that is returned after deploy is complete, but this section shows how you can create a predictor from an existing endpoint for inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker import Predictor\r\n",
    "from sagemaker.serializers import IdentitySerializer\r\n",
    "from sagemaker.deserializers import JSONDeserializer\r\n",
    "\r\n",
    "#Update the below variable with your endpoint name from previous cell output\r\n",
    "tf_endpoint_name='<SAGEMAKER DEPLOYED ENDPOINT NAME>'\r\n",
    "\r\n",
    "serializer = IdentitySerializer(content_type=\"application/x-image\")\r\n",
    "deserializer = JSONDeserializer(accept='application/json')\r\n",
    "\r\n",
    "predictor = Predictor(endpoint_name=tf_endpoint_name,serializer = serializer,deserializer = deserializer )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get trained Classes Info"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv_utils\r\n",
    "\r\n",
    "classes_file = f\"s3://{bucket}/{prefix}/full/data/classes.txt\"\r\n",
    "classes = [13, 17, 35, 36, 47, 68, 73, 87]\r\n",
    "\r\n",
    "possible_classes= cv_utils.get_classes_as_list(classes_file,classes)\r\n",
    "\r\n",
    "possible_classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download sample test images from S3 for inference\n",
    "\n",
    "This cell downloads a random number of images (specified by value of 'n') from 'test' data set and use them for running inferences using our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_images = cv_utils.get_n_random_images(bucket,prefix=f'{prefix}/outputs/test',n=2)\r\n",
    "\r\n",
    "local_paths = cv_utils.download_images_locally(bucket,sample_images)\r\n",
    "print(local_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Predictions from local images\n",
    "\n",
    "This cell will make use of the \"predict_bird_from_file\" utility function in order to both visualize the test image and respective bird class predicted by the model with its correspondent confidence score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for inputfile in local_paths:\r\n",
    "    print(inputfile)\r\n",
    "    cv_utils.predict_bird_from_file(inputfile,predictor,possible_classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean Up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}