{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Model Preparation\n",
    "This notebook will prepare the dataset and model for the module evaluation lab.  This is an optional step if you have kept your artifacts from previous modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and initialize parameters for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'postprocessing-modal-evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset we are using is from [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found here). Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels. Bounding boxes are provided, as are annotations of bird parts. A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "Run the cell below to download the full dataset or download manually [here](https://course.fast.ai/datasets). Note that the file size is around 1.2 GB, and can take a while to download. If you plan to complete the entire workshop, please keep the file to avoid re-download and re-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-26 12:44:54--  https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.88.110\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.88.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1150585339 (1.1G) [application/x-tar]\n",
      "Saving to: ‘CUB_200_2011.tgz’\n",
      "\n",
      "CUB_200_2011.tgz    100%[===================>]   1.07G  17.2MB/s    in 40s     \n",
      "\n",
      "2022-03-26 12:45:35 (27.2 MB/s) - ‘CUB_200_2011.tgz’ saved [1150585339/1150585339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz'\n",
    "!tar xopf CUB_200_2011.tgz\n",
    "!rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test samples for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = []\n",
    "image_folder = 'CUB_200_2011/images'\n",
    "dst = 'build/image_classification/images'\n",
    "\n",
    "for sub_dir in (glob.glob(f'{image_folder}/*')):\n",
    "    for filename in (glob.glob(f'{sub_dir}/*')):\n",
    "        img_array.append(filename)\n",
    "\n",
    "for i in range(10):\n",
    "    rand_index = random.randint(0,len(img_array)-1)\n",
    "    shutil.copy(img_array[rand_index], dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_data = f's3://{bucket}/{prefix}/full/data'\n",
    "!aws s3 cp --recursive ./CUB_200_2011 $s3_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./CUB_200_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "import time \n",
    "\n",
    "timpstamp = str(time.time()).split('.')[0]\n",
    "# SKlearnProcessor for preprocessing\n",
    "output_prefix = f'{prefix}/outputs'\n",
    "output_s3_uri = f's3://{bucket}/{output_prefix}'\n",
    "\n",
    "class_selection = '13, 17, 35, 36, 47, 68, 73, 87'\n",
    "input_annotation = 'classes.txt'\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "processing_instance_count = 1\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(base_job_name = f\"{prefix}-preprocess\",  # choose any name\n",
    "                                    framework_version='0.20.0',\n",
    "                                    role=role,\n",
    "                                    instance_type=processing_instance_type,\n",
    "                                    instance_count=processing_instance_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(\n",
    "    code='../02_preprocessing/preprocessing.py',\n",
    "    arguments=[\"--classes\", class_selection, \n",
    "               \"--input-data\", input_annotation],\n",
    "    inputs=[ProcessingInput(source=s3_raw_data, \n",
    "            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/train\", destination = output_s3_uri +'/train'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/valid\", destination = output_s3_uri +'/valid'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/test\", destination = output_s3_uri +'/test'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/manifest\", destination = output_s3_uri +'/manifest'),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where your images and annotation files are located.  You will need these for this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset located here: s3://sagemaker-us-west-2-987720697751/postprocessing-modal-evaluation/outputs/test ===========\n",
      "Test annotation file is located here: s3://sagemaker-us-west-2-987720697751/postprocessing-modal-evaluation/outputs/manifest ===========\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test dataset located here: {output_s3_uri +'/test'} ===========\")\n",
    "\n",
    "print(f\"Test annotation file is located here: {output_s3_uri +'/manifest'} ===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "TF_FRAMEWORK_VERSION = '2.1'\n",
    "\n",
    "hyperparameters = {'initial_epochs':     5,\n",
    "                   'batch_size':         8,\n",
    "                   'fine_tuning_epochs': 20, \n",
    "                   'dropout':            0.4,\n",
    "                   'data_dir':           '/opt/ml/input/data'}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',      'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'acc',       'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_loss',  'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_acc',   'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "\n",
    "distribution = {'parameter_server': {'enabled': False}}\n",
    "DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "train_in = TrainingInput(s3_data=output_s3_uri +'/train', distribution=DISTRIBUTION_MODE)\n",
    "val_in   = TrainingInput(s3_data=output_s3_uri +'/valid', distribution=DISTRIBUTION_MODE)\n",
    "test_in  = TrainingInput(s3_data=output_s3_uri +'/test', distribution=DISTRIBUTION_MODE)\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}\n",
    "\n",
    "training_instance_type = 'ml.c5.4xlarge'\n",
    "\n",
    "training_instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "               source_dir='code',\n",
    "               output_path=model_path,\n",
    "               instance_type=training_instance_type,\n",
    "               instance_count=training_instance_count,\n",
    "               distribution=distribution,\n",
    "               hyperparameters=hyperparameters,\n",
    "               metric_definitions=metric_definitions,\n",
    "               role=role,\n",
    "               framework_version=TF_FRAMEWORK_VERSION, \n",
    "               py_version='py3',\n",
    "               base_job_name=prefix,\n",
    "               script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "\n",
    "print(f\"model artifacts file is uploaded here: {model_path}/{training_job_name}/output ========\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
