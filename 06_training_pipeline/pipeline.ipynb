{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build an CV Training Pipeline using SageMaker Pipeline\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Dataset](#Dataset)\n",
    "5. [Build SageMaker Pipeline](#Build-SageMaker-Pipeline)\n",
    "    1. [Bring Your Own Container (BYOC)](#Bring-Your-Own-Container-(BYOC))\n",
    "    2. [Set Pipeline input parameters](#Set-Pipeline-input-parameters)\n",
    "    3. [Define Cache Configuration](#Define-Cache-Configuration)\n",
    "    4. [Preprocess data step](#Preprocess-data-step)\n",
    "    5. [Training step](#Training-step)\n",
    "    6. [Model evaluation step](#Model-evaluation-step)\n",
    "    7. [Register model step](#Register-model-step)\n",
    "    8. [Accuracy condition step](#Accuracy-condition-step)\n",
    "    9. [Pipeline Creation](#Pipeline-Creation)\n",
    "    10. [Submit and trig pipeline](#Submit-and-trig-pipeline)\n",
    "    11. [Analyzing Results](#Analyzing-Results)\n",
    "6. [Execute same pipeline in one continuous script](#Execute-same-pipeline-in-one-continuous-script)\n",
    "7. [Build Custom Project Templates (Optional)](#Build-Custom-Project-Templates-(Optional))\n",
    "    1. [Setup Service Catalog Portfolio](#Setup-Service-Catalog-Portfolio)\n",
    "7. [Clean Up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrate how to build a reusable computer vision (CV) pattern using **SageMaker Pipeline**. This particular pattern goes through preprocessing, training, and evaluating steps for 2 different training jobs:1) Spot training and 2) On Demand training.  If the accuracy meets certain requirements, the models are then registered with SageMaker Model Registry.\n",
    "\n",
    "We have also tagged the training workloads: `TrainingType: Spot or OnDemand`.  If you are interested and have permission to access billing of your AWS account, you the cost savings from spot training from the side-by-side comparison. To enable custom cost allocation tags, please follow this [AWS documentation](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html).  It takes 12-48 hrs for the new tag to show in your cost explore.\n",
    "\n",
    "![Spot Training](statics/cost-explore.png)\n",
    "\n",
    "SageMaker pipelines works on the concept of steps. The order steps are executed in is inferred from the dependencies each step have. If a step has a dependency on the output from a previous step, it's not executed until after that step has completed successfully. This also allows SageMaker to create a **Direct Acyclic Graph, DAG,** that can be visuallized in Amazon SageMaker Studio (see diagram above). The DAG can be used to track pipeline executions, inputs/outputs and metrics, giving user the full lineage of the model creation.\n",
    "\n",
    "![Training Pipeline](statics/cv-training-pipeline.png)\n",
    "\n",
    "** Note: This Notebook was tested on Data Science Kernel in SageMaker Studio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run this notebook, you can simply execute each cell in order. To understand what's happening, you'll need:\n",
    "\n",
    "- Access to the SageMaker default S3 bucket\n",
    "- Access to Elastic Container Registry (ECR)\n",
    "- For the optional portion of this lab, you will need access to CloudFormation, Service Catelog, and Cost Explore\n",
    "- Familiarity with Training on Amazon SageMaker\n",
    "- Familiarity with Python\n",
    "- Familiarity with AWS S3\n",
    "- Basic understanding of CloudFormaton and concept of deploy infra as code\n",
    "- Basic understanding of tagging and cost governance\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define the sagemaker session, default bucket, job prefixes, pipeline and model group names\n",
    "\n",
    "We are using some of the newly released SageMaker Pipeline features.  Please make sure you ugrade your sageMaker version by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker --quiet # Ensure latest version of SageMaker is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "base_job_prefix = \"cv-week4\" # or define your own prefix\n",
    "\n",
    "model_package_group_name = f\"{base_job_prefix}-model-group\"  # Model name in model registry\n",
    "pipeline_name = f\"{base_job_prefix}-pipeline\"  # SageMaker Pipeline name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset we are using is from [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found here). Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels. Bounding boxes are provided, as are annotations of bird parts. A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![Bird Dataset](statics/birds.png)\n",
    "\n",
    "Run the cell below to download the full dataset or download manually [here](https://course.fast.ai/datasets). Note that the file size is around 1.2 GB, and can take a while to download. If you plan to complete the entire workshop, please keep the file to avoid re-download and re-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-29 01:13:59--  https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.42.134\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.42.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1150585339 (1.1G) [application/x-tar]\n",
      "Saving to: ‘CUB_200_2011.tgz’\n",
      "\n",
      "CUB_200_2011.tgz    100%[===================>]   1.07G  25.5MB/s    in 41s     \n",
      "\n",
      "2022-08-29 01:14:40 (27.0 MB/s) - ‘CUB_200_2011.tgz’ saved [1150585339/1150585339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz'\n",
    "!tar xopf CUB_200_2011.tgz\n",
    "!rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# upload data to s3\n",
    "s3_raw_data = f's3://{default_bucket}/{base_job_prefix}/full/data'\n",
    "!aws s3 cp --recursive ./CUB_200_2011 $s3_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data to savve disc space\n",
    "!rm -rf ./CUB_200_2011\n",
    "!rm -f attributes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture the ECR URI here, we may use it later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Pipeline input parameters\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "![Parameter Input](statics/parameters-input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# Parameters for pipeline execution\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputDataUrl\",\n",
    "    default_value=s3_raw_data\n",
    ")\n",
    "\n",
    "input_annotation = ParameterString(\n",
    "    name=\"AnnotationFileName\",\n",
    "    default_value=\"classes.txt\"\n",
    ")\n",
    "\n",
    "# This is a large dataset, we are only going to train a subset of the classes\n",
    "class_selection = ParameterString(\n",
    "    name=\"ClassSelection\",\n",
    "    default_value=\"13, 17, 35, 36, 47, 68, 73, 87\" #If use the mini dataset, please make sure to use the class index with the available list\n",
    ")\n",
    "\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_count = 1\n",
    "training_instance_type = \"ml.c5.4xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cache Configuration\n",
    "When step cache is defined, before SageMaker Pipelines executes a step, it attempts to find a previous execution of a step that was called with the same arguments.\n",
    "\n",
    "Pipelines doesn't check whether the data or code that the arguments point to has changed. If a previous execution is found, Pipelines will propagates the values from the cache hit during execution, rather than recomputing the step.\n",
    "\n",
    "Step caching is available for the following step types:\n",
    "\n",
    "* Training\n",
    "* Tuning\n",
    "* Processing\n",
    "* Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "## By enabling cache, if you run this pipeline again, without changing the input \n",
    "## parameters it will skip the training part and reuse the previous trained model\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data step\n",
    "We are taking the original code in Jupyter notebook and containerized script to run in a preprocessing job.\n",
    "\n",
    "This script first down sample the dataset base on the ClassSelection parameter argument, then uses the manifest file (classes.txt) and split the data into train, validate, and test and store the data and manifest files to s3 bucket.\n",
    "\n",
    "We are going to use **SKLearnProcessor** to process the data. for more detail on different type of processing jobs, please refer to the amazon documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)\n",
    "\n",
    "1. build the preprocess python script (define s3 input and output)\n",
    "2. build the custom container\n",
    "3. Run sagemaker processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "# SKlearnProcessor for preprocessing\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(base_job_name = f\"{base_job_prefix}-preprocess\",  # choose any name\n",
    "                                    framework_version='0.20.0',\n",
    "                                    role=role,\n",
    "                                    instance_type=processing_instance_type,\n",
    "                                    instance_count=processing_instance_count)\n",
    "\n",
    "output_s3_uri = f's3://{default_bucket}/{base_job_prefix}/outputs/{uuid.uuid4()}'\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"BirdClassificationPreProcess\",  # choose any name\n",
    "    processor=sklearn_processor,\n",
    "    code=\"preprocess.py\",\n",
    "    job_arguments=[\"--classes\", class_selection,\n",
    "                \"--input-data\", input_annotation],\n",
    "    inputs=[ProcessingInput(source=input_data, \n",
    "            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train_data', \n",
    "                         source=\"/opt/ml/processing/output/train\", \n",
    "                         destination = output_s3_uri +'/train'),\n",
    "        ProcessingOutput(output_name='val_data',\n",
    "                         source=\"/opt/ml/processing/output/validation\", \n",
    "                         destination = output_s3_uri +'/validation'),\n",
    "        ProcessingOutput(output_name='test_data',\n",
    "                         source=\"/opt/ml/processing/output/test\", \n",
    "                         destination = output_s3_uri +'/test'),\n",
    "        ProcessingOutput(output_name='manifest',\n",
    "                         source=\"/opt/ml/processing/output/manifest\", \n",
    "                         destination = output_s3_uri +'/manifest'),\n",
    "    ],\n",
    "    cache_config=cache_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "We are using SageMaker's TensorFlow container, the custom TensorFlow training code is provided via a Python script in a separate file that gets passed to SageMaker.\n",
    "\n",
    "Our Pipeline experiments with 2 training jobs, Spot and On Demand, side-by-side.  Each workload is tagged using 'TrainingType'.  It you have the permission, you can enable the User defined tag in Cost Explore and compare the cost difference between spot and on demand training.  [Here](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/activating-tags.html) is how to enable user-defined tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "TF_FRAMEWORK_VERSION = '2.4.1'\n",
    "\n",
    "hyperparameters = {'initial_epochs':     5,\n",
    "                   'batch_size':         8,\n",
    "                   'fine_tuning_epochs': 20, \n",
    "                   'dropout':            0.4,\n",
    "                   'data_dir':           '/opt/ml/input/data'}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',      'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'acc',       'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_loss',  'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "                  {'Name': 'val_acc',   'Regex': 'val_accuracy: ([0-9\\\\.]+)'}]\n",
    "\n",
    "if training_instance_count > 1:\n",
    "    distribution = {'parameter_server': {'enabled': True}}\n",
    "    DISTRIBUTION_MODE = 'ShardedByS3Key'\n",
    "else:\n",
    "    distribution = {'parameter_server': {'enabled': False}}\n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "train_in = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "test_in  = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "val_in   = TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"val_data\"].S3Output.S3Uri,\n",
    "                         distribution=DISTRIBUTION_MODE)\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/steps.py:444: UserWarning: Profiling is enabled on the provided estimator. The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "training_steps = dict()\n",
    "\n",
    "training_estimators = dict()\n",
    "# get out model artifacts location\n",
    "models = dict()\n",
    "\n",
    "training_options = ['Spot', 'OnDemand']\n",
    "\n",
    "for t in training_options:\n",
    "    tags = dict()\n",
    "    tags['Key'] = 'TrainingType'\n",
    "    tags['Value'] = t\n",
    "        # Training step for generating model artifacts\n",
    "    model_path = f\"s3://{default_bucket}/{base_job_prefix}/output/models\"\n",
    "    checkpoint_s3_uri = f\"s3://{default_bucket}/{base_job_prefix}/outputcheckpoints\"\n",
    "    \n",
    "    if t.lower() == 'spot':\n",
    "        estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "                               source_dir='code',\n",
    "                               output_path=model_path,\n",
    "                               instance_type=training_instance_type,\n",
    "                               instance_count=training_instance_count,\n",
    "                               distribution=distribution,\n",
    "                               hyperparameters=hyperparameters,\n",
    "                               metric_definitions=metric_definitions,\n",
    "                               role=role,\n",
    "                               use_spot_instances=True,\n",
    "                               max_run=60*60*10,\n",
    "                               max_wait=60*60*12, # Seconds to wait for spot instances to become available\n",
    "                               checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                               framework_version=TF_FRAMEWORK_VERSION, \n",
    "                               py_version='py37',\n",
    "                               base_job_name=base_job_prefix,\n",
    "                               script_mode=True,\n",
    "                               tags=[tags])\n",
    "    else:\n",
    "        estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "                       source_dir='code',\n",
    "                       output_path=model_path,\n",
    "                       instance_type=training_instance_type,\n",
    "                       instance_count=training_instance_count,\n",
    "                       distribution=distribution,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=role,\n",
    "                       framework_version=TF_FRAMEWORK_VERSION, \n",
    "                       py_version='py37',\n",
    "                       base_job_name=base_job_prefix,\n",
    "                       script_mode=True,\n",
    "                       tags=[tags])\n",
    "        \n",
    "    step_train = TrainingStep(\n",
    "        name=f\"BirdClassification{t}Train\",\n",
    "        estimator=estimator,\n",
    "        inputs=inputs,\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "    \n",
    "    training_steps[t] = step_train\n",
    "    training_estimators[t] = estimator\n",
    "    models[t] = step_train.properties.ModelArtifacts.S3ModelArtifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation step\n",
    "We are going to use a ProcessingStep for our model evaluation, and we are going to use our own container from the earlier step.\n",
    "\n",
    "Evaluation script does the following:\n",
    "1. Loading the tf model \n",
    "2. Run prediction\n",
    "3. Compare predicts vs actuals and generate the confussion matrix\n",
    "\n",
    "when you register this model to the model registery, metrics generated from this step will be attached to the model version and can be visualized in SageMaker Studio like this:\n",
    "\n",
    "![Confusion Matrix](statics/confussion_matrix.png)\n",
    "\n",
    "Here are more details on the list of metric available for each type of ML problems: [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  cv-week4-evaluation-2022-08-29-01-51-22-926\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c3869bd0>, 'LocalPath': '/opt/ml/processing/input/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c104f9d0>, 'LocalPath': '/opt/ml/processing/input/manifest', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-3', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c24314d0>, 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-22-926/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-22-926/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-22-926/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n",
      "\n",
      "Job Name:  cv-week4-evaluation-2022-08-29-01-51-23-327\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c3869bd0>, 'LocalPath': '/opt/ml/processing/input/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c104f9d0>, 'LocalPath': '/opt/ml/processing/input/manifest', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-3', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fc1c1a3be90>, 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-23-327/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-23-327/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-01-51-23-327/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "# from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    FrameworkProcessor,\n",
    ")\n",
    "\n",
    "eval_steps = dict()\n",
    "eval_reports = dict()\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "for t in training_options:\n",
    "    \n",
    "    script_eval = FrameworkProcessor(\n",
    "        estimator_cls=TensorFlow,\n",
    "        framework_version=TF_FRAMEWORK_VERSION,\n",
    "        base_job_name = f\"{base_job_prefix}-evaluation\",\n",
    "        command=['python3'],\n",
    "        py_version=\"py37\",\n",
    "        role=role,\n",
    "        instance_count=processing_instance_count,\n",
    "        instance_type=processing_instance_type,\n",
    "        sagemaker_session = pipeline_session)\n",
    "\n",
    "    \n",
    "    step_args = script_eval.run(\n",
    "        code='evaluation.py',\n",
    "        arguments=[\"--model-file\", \"model.tar.gz\"],\n",
    "        inputs=[ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri, \n",
    "                                destination=\"/opt/ml/processing/input/test\"),\n",
    "                ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[\"manifest\"].S3Output.S3Uri, \n",
    "                                destination=\"/opt/ml/processing/input/manifest\"),\n",
    "                ProcessingInput(source=models[t], \n",
    "                                destination=\"/opt/ml/processing/model\"),\n",
    "               ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=f\"Evaluation{t}Report\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=f\"BirdClassification{t}Eval\",\n",
    "        step_args = step_args,\n",
    "        property_files=[evaluation_report],\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "    \n",
    "    eval_steps[t] = step_eval\n",
    "    eval_reports[t] = evaluation_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model step\n",
    "In this step, the resulting model artifacts is register as a model package in a model package group. \n",
    "\n",
    "A model package is a reusable model artifacts abstraction that packages all ingredients required for inference. It also captures the metrics from the evaluation step for future comparison.\n",
    "\n",
    "A model package group is a collection of model packages, sually different model versions.  It also enables the user to compare metric accross different models.  \n",
    "\n",
    "Specifically, pass in the S3ModelArtifacts from the TrainingStep, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "model_register_steps = dict()\n",
    "\n",
    "for t in training_options:\n",
    "    # Create ModelMetrics object using the evaluation report from the evaluation step\n",
    "    # A ModelMetrics object contains metrics captured from a model.\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                eval_steps[t].arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "                    \"S3Uri\"\n",
    "                ]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Crete a RegisterModel step, which registers the model with Sagemaker Model Registry.\n",
    "    step_register = RegisterModel(\n",
    "        name=f\"Register{t}Model\",\n",
    "        estimator=training_estimators[t],\n",
    "        model_data=models[t],\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "    \n",
    "    model_register_steps[t] = step_register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy condition step\n",
    "This condition step only allows the model to be registered if the accuracy of the model, as determined by the evaluation step step_eval, exceeded a specified value. A ConditionStep enables pipelines to support conditional execution in the pipeline DAG based on the conditions of the step properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "condition_steps = dict()\n",
    "\n",
    "for t in training_options:\n",
    "    \n",
    "    # Create accuracy condition to ensure the model meets performance requirements.\n",
    "    # Models with a test accuracy lower than the condition will not be registered with the model registry.\n",
    "    cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "        left=JsonGet(\n",
    "            step=eval_steps[t],\n",
    "            property_file=eval_reports[t],\n",
    "            json_path=\"multiclass_classification_metrics.accuracy.value\",\n",
    "        ),\n",
    "        right=0.7,\n",
    "    )\n",
    "\n",
    "    # Create a Sagemaker Pipelines ConditionStep, using the condition above.\n",
    "    # Enter the steps to perform if the condition returns True / False.\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"BirdClassification{t}Condition\",\n",
    "        conditions=[cond_gte],\n",
    "        if_steps=[model_register_steps[t]],\n",
    "        else_steps=[],\n",
    "    )\n",
    "    \n",
    "    condition_steps[t] = step_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Creation\n",
    "\n",
    "Last step is to combine all the previous steps into a Pipeline so it can be executed.\n",
    "\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Create a Sagemaker Pipeline.\n",
    "# Each parameter for the pipeline must be set as a parameter explicitly when the pipeline is created.\n",
    "\n",
    "# build the steps\n",
    "steps = [step_process]\n",
    "for t in training_steps:\n",
    "    steps.append(training_steps[t])\n",
    "    \n",
    "for e in eval_steps:\n",
    "    steps.append(eval_steps[e])\n",
    "    \n",
    "for c in condition_steps:\n",
    "    steps.append(condition_steps[c])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        input_annotation,\n",
    "        class_selection\n",
    "    ],\n",
    "    steps=steps,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and trig pipeline\n",
    "Submit the pipeline definition to the Pipeline service. The role passed in will be used by the Pipeline service to create all the jobs defined in the steps.\n",
    "\n",
    "Once a pipeline has been submited (pipeline.upsert()), user can trigger the pipeline using the API (pipeline.start()) or through the SageMaker Studo UI:\n",
    "\n",
    "![Pipeline UI Trigger](statics/studio-ui-pipeline.png)\n",
    "\n",
    "![Pipeline Code Trigger](statics/execute-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "# Submit pipline\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Execute pipeline using the default parameters.\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Results\n",
    "You can compre different version of model by selecting multiple versions and right-click -> Compare model versions.  If you have visuallizations, you graph may overlap depending on how complete your use case is.\n",
    "\n",
    "![Model Comparison](statics/compare-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute same pipeline in one continuous script\n",
    "To operationalize this pipeline, we also provide this code in a continuous script.  Please review this pipeline.py file.  Here is the code to execute the script to build and run a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "base_job_prefix = \"cv-week4\" # or define your own prefix\n",
    "\n",
    "model_package_group_name2 = f\"{base_job_prefix}-model-group2\"  # Model name in model registry\n",
    "pipeline_name2 = f\"{base_job_prefix}-pipeline2\"  # SageMaker Pipeline name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how to load the pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/steps.py:444: UserWarning: Profiling is enabled on the provided estimator. The default profiler rule includes a timestamp which will change each time the pipeline is upserted, causing cache misses. If profiling is not needed, set disable_profiler to True on the estimator.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:236: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n",
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  cv-week4-evaluation-2022-08-29-02-32-14-160\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab9ab24b10>, 'LocalPath': '/opt/ml/processing/input/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab98200d50>, 'LocalPath': '/opt/ml/processing/input/manifest', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-3', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab98cb0b50>, 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-160/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-160/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-160/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  cv-week4-evaluation-2022-08-29-02-32-14-774\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab9ab24b10>, 'LocalPath': '/opt/ml/processing/input/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab98200d50>, 'LocalPath': '/opt/ml/processing/input/manifest', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-3', 'AppManaged': False, 'S3Input': {'S3Uri': <sagemaker.workflow.properties.Properties object at 0x7fab96d42210>, 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-774/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-774/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-987720697751/cv-week4-evaluation-2022-08-29-02-32-14-774/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from pipeline import get_pipeline\n",
    "\n",
    "pipeline2 = get_pipeline(\n",
    "    region=region,\n",
    "    role=role,\n",
    "    default_bucket=default_bucket,\n",
    "    model_package_group_name=model_package_group_name2,\n",
    "    pipeline_name=pipeline_name2,\n",
    "    base_job_prefix=base_job_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to submit/update the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:987720697751:pipeline/cv-week4-pipeline2',\n",
       " 'ResponseMetadata': {'RequestId': '805f62e8-6f9b-48a5-b134-a49a1b7c8b8f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '805f62e8-6f9b-48a5-b134-a49a1b7c8b8f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '86',\n",
       "   'date': 'Mon, 29 Aug 2022 02:32:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to Run the pipeline and overwrite the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline2.start(\n",
    "    parameters=dict(\n",
    "        InputDataUrl=s3_raw_data, # loaction of the raw data\n",
    "        ProcessingInstanceCount=1,\n",
    "#         ProcessingInstanceType=\"ml.m5.xlarge\",\n",
    "#         TrainingInstanceCount=1,\n",
    "#         TrainingInstanceType=\"ml.c5.4xlarge\",#\"ml.p3.2xlarge\",#\n",
    "        ModelApprovalStatus=\"PendingManualApproval\",\n",
    "        AnnotationFileName=\"classes.txt\",\n",
    "        ClassSelection=\"13, 17, 35, 36\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Project Templates (Optional)\n",
    "<span style=\"color:red\">Note: You will need some DevOps permissions like Service Catalog and Cloudformation template to run this portion of the lab </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Amazon SageMaker project template automates the set up and implementation of MLOps for your projects. SageMaker already provides a set of project templates that create the infrastructure you need to create an MLOps solution for CI/CD of your ML models. [More on Provided Project Templates](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-sm.html)  If the SageMaker provided templates do not meet your needs, you can also create your own templates.\n",
    "\n",
    "For that, you need the pipeline definition file.  It is a JSON file that defines a series of interconnected steps using Directed Acyclic Graph (DAG) and specifies the requirements and relationships between each step of your pipeline.  This JSON file along with Cloudformation template also allows you to manage your pipeline and it's underline infrastructure as code, so you can establish re-usable CI/CD patterns that standardizes across your teams and organizations.\n",
    "\n",
    "A CloudFormation template `sagemaker-project.yaml` is supplied with this week's lab.\n",
    "\n",
    "It will generate following resources:\n",
    "* S3 bucket - with Lambda PUT notification enabled\n",
    "* A lambda function - when triggered will create/update a SageMaker pipeline and execute it\n",
    "* A SageMaker pipeline - generated from the JSON definition file\n",
    "* A SageMaker Model Group - to house the complete models\n",
    "\n",
    "Note: for lab purpose, we are loose with our role permissions.  You will likely use tighter permission control than what's provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we want to upload the lambda and pipeline definition file to a s3 bucket. Capture the bucket name and object key\n",
    "\n",
    "Note this pipeline definition file contains the default paramter values you defined in your code.  You can customize those parameter when you execute this piepleine.  In this example, ECR image url is an input to the SageMaker project, and that is pushed to the lambda evnironment vairable to overwrite the default value during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# upload lambda.zip\n",
    "file_name = \"lambda/lambda.zip\"\n",
    "s3.upload_file(file_name, default_bucket, f\"{base_job_prefix}/lambda.zip\")\n",
    "\n",
    "print(f'LambdaBucket: {default_bucket}')\n",
    "print(f'LambdaKey: {base_job_prefix}/lambda.zip')\n",
    "\n",
    "# upload your piepline definition file generate from the pipeline of this lab\n",
    "\n",
    "s3.put_object(\n",
    "     Body=pipeline.definition(),\n",
    "     Bucket=default_bucket,\n",
    "     Key=f'{base_job_prefix}/pipeline-definition.json'\n",
    ")\n",
    "\n",
    "print(f'DefinitionBucket: {default_bucket}')\n",
    "print(f'DefinitionKey: {base_job_prefix}/pipeline-definition.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Service Catalog Portfolio\n",
    "\n",
    "1. Enable permissions to use SageMaker Projects.  [Instructions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-studio-updates.html) for **For admin and domain execution role user ONLY** \n",
    "2. Follow the step-by-step instruction [Here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-templates-custom.html) to create the Service Catalog product, and make it visible in SageMaker Studio:\n",
    "* Navigate to Service Catalog\n",
    "* Navigate to Portfolio and Create a Portfolio\n",
    "* Navigate to Product and Create a Product\n",
    "* Add Product to Portfolio created in step b\n",
    "* Add `sagemaker:studio-visibility`:`true` tag to make this product visible in Sagemaker Studio\n",
    "* Navigate to Portfilio and create a launch constraint for the new product\n",
    "* Under Group, Role, and user -> Add ... -> Roles -> **Select the SageMaker Execution Role** for your Studio environment.  This is required for Studio to access this template.\n",
    "---\n",
    "\n",
    "**Make sure the Launch constraint role has permission for IAM, S3, SageMaker, Lambda.**  For simplicity, I would make sure the LaunchConstraintRole has following:\n",
    "- IAMFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy\n",
    "- AWSLambda_FullAccess\n",
    "\n",
    "3. Now refresh Studio and navigate to project by \"Components & Registry icon\" -> Projects -> Create project -> Organization templates.  Your custom template should show up here\n",
    "\n",
    "![Studio Template](statics/project-template.png)\n",
    "\n",
    "4. Select your template.  fillout the Project Name, and **replace the rest of the parameters with your specific bucket name, s3 key, role name, etc. (Project Name is also used to create a new S3 bucket, so it has to been unique for project and bucket name)**\n",
    "\n",
    "![Studio Parameters](statics/project-parameter.png)\n",
    "\n",
    "5. Create Project.  You will get a project that encapsulate your pipeline and model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update and execute your pipeline, just need to drop a new definition json file to the new bucket you just created.  When a new json file is uploaded, it will automatically trigger the lambda function to update and run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bucket = \"cv-week4-demo\"#\"Same-As-Your-Project-Name\"#\"cv-week4-demo3\"#\n",
    "\n",
    "#create an new pipeline definition\n",
    "definition = json.loads(pipeline2.definition())\n",
    "\n",
    "for p in definition['Parameters']:\n",
    "    if p['Name'] == 'InputDataUrl':\n",
    "        p['DefaultValue'] = s3_raw_data\n",
    "    elif p['Name'] == 'EvaluationImage':\n",
    "        p['DefaultValue'] = ecr_image\n",
    "\n",
    "new_pipeline_definition = json.dumps(definition)\n",
    "\n",
    "s3.put_object(\n",
    "     Body=new_pipeline_definition,\n",
    "     Bucket=new_bucket,\n",
    "     Key=f'pipeline/definition-{uuid.uuid4()}.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Delete the model registry and the pipeline after you complete the lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model_package_group(sm_client, package_group_name):\n",
    "    try:\n",
    "        model_versions = sm_client.list_model_packages(ModelPackageGroupName=package_group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return\n",
    "\n",
    "    for model_version in model_versions[\"ModelPackageSummaryList\"]:\n",
    "        try:\n",
    "            sm_client.delete_model_package(ModelPackageName=model_version[\"ModelPackageArn\"])\n",
    "        except Exception as e:\n",
    "            print(\"{} \\n\".format(e))\n",
    "        time.sleep(0.5)  # Ensure requests aren't throttled\n",
    "\n",
    "    try:\n",
    "        sm_client.delete_model_package_group(ModelPackageGroupName=package_group_name)\n",
    "        print(\"{} model package group deleted\".format(package_group_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "    return\n",
    "\n",
    "\n",
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return\n",
    "    \n",
    "def delete_sagemaker_project(sm_client, project_name):\n",
    "    try:\n",
    "        sm_client.delete_project(\n",
    "            ProjectName=project_name,\n",
    "        )\n",
    "        print(\"{} project deleted\".format(project_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "\n",
    "delete_model_package_group(client, model_package_group_name)\n",
    "delete_sagemaker_pipeline(client, pipeline_name)\n",
    "\n",
    "delete_model_package_group(client, model_package_group_name2)\n",
    "delete_sagemaker_pipeline(client, pipeline_name2)\n",
    "\n",
    "# delete_sagemaker_project(client, \"<Your-Project-Name>\")#\"cv-week4-training\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
