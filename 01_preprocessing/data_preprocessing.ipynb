{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing using a SKLearn Processor\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> This notebook has been tested using the `Python 3 (Data Science)` kernel in SageMaker Studio.\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [The Raw Dataset](#The-Raw-Dataset)\n",
    "5. [The Data Labels](#The-Data-Labels)\n",
    "5. [Defining a SageMaker Processing Job](#Defining-a-SageMaker-Processing-Job)\n",
    "6. [Review Outputs](#Review-Outputs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Data processing tasks such as feature engineering, data validation, model evaluation, and model interpretation are essential steps performed by engineers and data scientists in this machine learning workflow.\n",
    "\n",
    "With Amazon SageMaker Processing jobs you can run custom scripts for all the above tasks in several popular frameworks such as Scikit learn and Spark. \n",
    "\n",
    "In this lab you will learn how to use [SKLearnProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html), a SageMaker library helper class that allows you to leverage a specific type of SageMaker processing container. The SKLearnProcessor uses scikit-learn scripts in a container image provided and maintained by AWS in order to preprocess data or evaluate models.\n",
    "\n",
    "![Process Data](https://docs.aws.amazon.com/sagemaker/latest/dg/images/Processing-1.png)\n",
    "\n",
    "The example script will:\n",
    "1. Load the bird dataset\n",
    "2. Split data into train, validation, and test channels\n",
    "3. Export the data and annotation files to S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\r\n",
    "\r\n",
    "Download the notebook into your environment, and you can run it by simply execute each cell in order. To understand what is happening, you will need:\r\n",
    "\r\n",
    "- Access to the SageMaker default S3 bucket. All the files related to this lab will be stored under the \"cv-sagemaker-immersionday\" prefix of the bucket.\r\n",
    "- Familiarity with Python and numpy\r\n",
    "- Basic familiarity with AWS S3.\r\n",
    "- Basic understanding of AWS Sagemaker.\r\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\r\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Setting up the environment, load the libraries, and define the parameter for the entire notebook.\n",
    "\n",
    "Run the cell below to ensure latest version of SageMaker is installed in your kernel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -U sagemaker --quiet # Ensure latest version of SageMaker is installed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "import boto3\r\n",
    "\r\n",
    "sagemaker_session = sagemaker.Session()\r\n",
    "region = sagemaker_session.boto_region_name\r\n",
    "account = sagemaker_session.account_id()\r\n",
    "role = sagemaker.get_execution_role()\r\n",
    "\r\n",
    "default_bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\r\n",
    "base_job_prefix = \"cv-sagemaker-immersionday\" # or define your own prefix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Raw Dataset\n",
    "The dataset we are using is the [Caltech Birds (CUB 200 2011)](https://www.vision.caltech.edu/datasets/cub_200_2011/) dataset.\n",
    "\n",
    "It contains 11,788 images across 200 bird species (the original technical report can be found [here](https://authors.library.caltech.edu/27452/)). \n",
    "\n",
    "Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels. \n",
    "\n",
    "Bounding boxes are provided, as are annotations of bird parts. \n",
    "\n",
    "A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![Bird Dataset](statics/birds.png)\n",
    "\n",
    "Run the cell below to download the full dataset from a public S3 location and unzip the folder structure. Note that the file size is around 1.2 GB, and can take a while to download. If you plan to complete the entire workshop, please keep the file to avoid re-download and re-process the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget 'https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz'\n",
    "!tar xopf CUB_200_2011.tgz\n",
    "!rm CUB_200_2011.tgz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the cell below to upload the unzipped dataset to your SageMaker default bucket."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_raw_data = f's3://{default_bucket}/{base_job_prefix}/full/data'\n",
    "!aws s3 cp --recursive ./CUB_200_2011 $s3_raw_data --quiet"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Data Labels \n",
    "\n",
    "The dataset comes with bird class labels. They are encoded in two files:\n",
    "\n",
    "    - `classes.txt` which gives the human-readable format of each class\n",
    "    - `image_class_labels.txt` which describes the class of each image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!head CUB_200_2011/classes.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!head CUB_200_2011/image_class_labels.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we would have not had the classes of the images, we could have used SageMaker Ground Truth to find the resources for labelling the data.\n",
    "\n",
    "Ground Truth is fully managed data labeling service in which you can launch a labeling job with just a few clicks in the console or use a single AWS SDK API call. \n",
    "\n",
    "It provides 30+ labeling workflows for computer vision and NLP use cases, and also allows you to tap into different workforce options.\n",
    "\n",
    "![SMGT](https://docs.aws.amazon.com/sagemaker/latest/dg/images/image-classification-example.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining a SageMaker Processing Job\n",
    "\n",
    "As mentioned before, we are going to practice using scikit-learn processing jobs. \n",
    "\n",
    "Because we are using a built-in SageMaker Scikit-learn container, the only thing you need to provide in addition is a Python script.\n",
    "\n",
    "Please inspect the [preprocessing.py](preprocessing.py) script that has been provided for you.\n",
    "\n",
    "The script:\n",
    "- takes in the raw images files and splits them into training, validation and test sets by class\n",
    "- merges the class annotation files so that you have a manifest file for each separate data set\n",
    "- exposes two parameters: classes (allows you to filter the number of classes you want to train the model on; default is all classes) and input-data (the human readable name of the classes)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "# SKlearnProcessor for preprocessing\n",
    "output_prefix = f'{base_job_prefix}/outputs'\n",
    "output_s3_uri = f's3://{default_bucket}/{output_prefix}'\n",
    "\n",
    "class_selection = '13, 17, 35, 36, 47, 68, 73, 87'\n",
    "input_annotation = 'classes.txt'\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "processing_instance_count = 1\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(base_job_name = f\"{base_job_prefix}-preprocess\",  # choose any name\n",
    "                                    framework_version='0.20.0',\n",
    "                                    role=role,\n",
    "                                    instance_type=processing_instance_type,\n",
    "                                    instance_count=processing_instance_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    arguments=[\"--classes\", class_selection, \n",
    "               \"--input-data\", input_annotation],\n",
    "    inputs=[ProcessingInput(source=s3_raw_data, \n",
    "            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/train\", destination = output_s3_uri +'/train'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/valid\", destination = output_s3_uri +'/valid'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/test\", destination = output_s3_uri +'/test'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/manifest\", destination = output_s3_uri +'/manifest'),\n",
    "        ],\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Review Outputs\n",
    "\n",
    "At the end of the lab, you dataset will be randomly split into train, valid, and test folders. You will also have a csv manifest file for each channel. \n",
    "\n",
    "Validate your results with the script below. \n",
    "\n",
    "**If you plan to complete other modules in this workshop, please keep these data.  Otherwise, you can clean up after this lab.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "response = s3_client.list_objects_v2(Bucket=default_bucket, Prefix=output_prefix)\n",
    "files = response.get(\"Contents\")\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(f\"file_name: {file['Key']}, size: {file['Size']}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}