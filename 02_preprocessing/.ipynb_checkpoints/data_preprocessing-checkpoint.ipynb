{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing using SKLearn Processor\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "3. [Setup](#Setup)\n",
    "4. [Dataset](#Dataset)\n",
    "5. [Build a SageMaker Processing Job](#Build-a-SageMaker-Processing-Job)\n",
    "    1. [Review Processcikit-learn Script](#Processcikit-Learn-scripts)\n",
    "    2. [Configure Processing Job](#Configure-Processing-Job)\n",
    "6. [Review Outputs](#Review-Outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Preprocess dataset before model training is an important step in the overall MLOps process. In this lab you will learn how to use [SKLearnProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html), a type of SageMaker process uses Processcikit-learn scripts in a container image provided and maintained by SageMaker to preprocess data or evaluate models.\n",
    "\n",
    "The example script will first Load the bird dataset, and then split data into train, validation, and test channels, and finally Export the data and annotation files to S3.\n",
    "\n",
    "\n",
    "** Note: This Notebook was tested on Data Science Kernel in SageMaker Studio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Download the notebook into your environment, and you can run it by simply execute each cell in order. To understand what's happening, you'll need:\n",
    "\n",
    "- Access to the SageMaker default S3 bucket. All the files related to this lab will be stored under the \"cv_keras_cifar10\" prefix of the bucket.\n",
    "- Familiarity with Python and numpy\n",
    "- Basic familiarity with AWS S3.\n",
    "- Basic understanding of AWS Sagemaker.\n",
    "- Basic familiarity with AWS Command Line Interface (CLI) -- ideally, you should have it set up with credentials to access the AWS account you're running this notebook from.\n",
    "- SageMaker Studio is preferred for the full UI integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Setting up the environment, load the libraries, and define the parameter for the entire notebook.\n",
    "\n",
    "Run the cell below to ensure latest version of SageMaker is installed in your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker --quiet # Ensure latest version of SageMaker is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "account = sagemaker_session.account_id()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "base_job_prefix = \"preprocess\" # or define your own prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset we are using is from [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found here). Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels. Bounding boxes are provided, as are annotations of bird parts. A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![Bird Dataset](statics/birds.png)\n",
    "\n",
    "Run the cell below to download the full dataset or download manually [here](https://course.fast.ai/datasets). Note that the file size is around 1.2 GB, and can take a while to download. If you plan to complete the entire workshop, please keep the file to avoid re-download and re-process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz'\n",
    "!tar xopf CUB_200_2011.tgz\n",
    "!rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload to S3 and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_raw_data = f's3://{default_bucket}/{base_job_prefix}/full/data'\n",
    "!aws s3 cp --recursive ./CUB_200_2011 $s3_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a SageMaker Processing Job\n",
    "There are 3 types of processing job depanding on which framework you want to use: [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)\n",
    "\n",
    "For this example, we are going to practice using scikit-learn processing.  This will use SageMaker built-in Scikit-learn container, so all you need to provide is a python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processcikit Learn scripts\n",
    "The script takes in the raw images files and split them into training, validation and test set by class.  It also split the anotation file so you have a manifest file for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "input_path = \"/opt/ml/processing/input\" #\"CUB_200_2011\" # \n",
    "output_path = '/opt/ml/processing/output' #\"output\" # \n",
    "IMAGES_DIR   = os.path.join(input_path, 'images')\n",
    "SPLIT_RATIOS = (0.6, 0.2, 0.2)\n",
    "\n",
    "\n",
    "# this function is used to split a dataframe into 3 seperate dataframes\n",
    "# one of each: train, validate, test\n",
    "\n",
    "def split_to_train_val_test(df, label_column, splits=(0.7, 0.2, 0.1), verbose=False):\n",
    "    train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    labels = df[label_column].unique()\n",
    "    for lbl in labels:\n",
    "        lbl_df = df[df[label_column] == lbl]\n",
    "\n",
    "        lbl_train_df        = lbl_df.sample(frac=splits[0])\n",
    "        lbl_val_and_test_df = lbl_df.drop(lbl_train_df.index)\n",
    "        lbl_test_df         = lbl_val_and_test_df.sample(frac=splits[2]/(splits[1] + splits[2]))\n",
    "        lbl_val_df          = lbl_val_and_test_df.drop(lbl_test_df.index)\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n{}:\\n---------\\ntotal:{}\\ntrain_df:{}\\nval_df:{}\\ntest_df:{}'.format(lbl,\n",
    "                                                                        len(lbl_df), \n",
    "                                                                        len(lbl_train_df), \n",
    "                                                                        len(lbl_val_df), \n",
    "                                                                        len(lbl_test_df)))\n",
    "        train_df = train_df.append(lbl_train_df)\n",
    "        val_df   = val_df.append(lbl_val_df)\n",
    "        test_df  = test_df.append(lbl_test_df)\n",
    "\n",
    "    # shuffle them on the way out using .sample(frac=1)\n",
    "    return train_df.sample(frac=1), val_df.sample(frac=1), test_df.sample(frac=1)\n",
    "\n",
    "# This function grabs the manifest files and build a dataframe, then call the split_to_train_val_test\n",
    "# function above and return the 3 dataframes\n",
    "def get_train_val_dataframes(BASE_DIR, classes, split_ratios):\n",
    "    CLASSES_FILE = os.path.join(BASE_DIR, 'classes.txt')\n",
    "    IMAGE_FILE   = os.path.join(BASE_DIR, 'images.txt')\n",
    "    LABEL_FILE   = os.path.join(BASE_DIR, 'image_class_labels.txt')\n",
    "\n",
    "    images_df = pd.read_csv(IMAGE_FILE, sep=' ',\n",
    "                            names=['image_pretty_name', 'image_file_name'],\n",
    "                            header=None)\n",
    "    image_class_labels_df = pd.read_csv(LABEL_FILE, sep=' ',\n",
    "                                names=['image_pretty_name', 'orig_class_id'], header=None)\n",
    "\n",
    "    # Merge the metadata into a single flat dataframe for easier processing\n",
    "    full_df = pd.DataFrame(images_df)\n",
    "\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "    full_df = pd.merge(full_df, image_class_labels_df, on='image_pretty_name')\n",
    "\n",
    "    # grab a small subset of species for testing\n",
    "    criteria = full_df['orig_class_id'].isin(classes)\n",
    "    full_df = full_df[criteria]\n",
    "    print('Using {} images from {} classes'.format(full_df.shape[0], len(classes)))\n",
    "\n",
    "    unique_classes = full_df['orig_class_id'].drop_duplicates()\n",
    "    sorted_unique_classes = sorted(unique_classes)\n",
    "    id_to_one_based = {}\n",
    "    i = 1\n",
    "    for c in sorted_unique_classes:\n",
    "        id_to_one_based[c] = str(i)\n",
    "        i += 1\n",
    "\n",
    "    full_df['class_id'] = full_df['orig_class_id'].map(id_to_one_based)\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    def get_class_name(fn):\n",
    "        return fn.split('/')[0]\n",
    "    full_df['class_name'] = full_df['image_file_name'].apply(get_class_name)\n",
    "    full_df = full_df.drop(['image_pretty_name'], axis=1)\n",
    "\n",
    "    train_df = []\n",
    "    test_df  = []\n",
    "    val_df   = []\n",
    "\n",
    "    # split into training and validation sets\n",
    "    train_df, val_df, test_df = split_to_train_val_test(full_df, 'class_id', split_ratios)\n",
    "\n",
    "    print('num images total: ' + str(images_df.shape[0]))\n",
    "    print('\\nnum train: ' + str(train_df.shape[0]))\n",
    "    print('num val: ' + str(val_df.shape[0]))\n",
    "    print('num test: ' + str(test_df.shape[0]))\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# this function copy images by channel to its destination folder\n",
    "def copy_files_for_channel(df, channel_name, verbose=False):\n",
    "    print('\\nCopying files for {} images in channel: {}...'.format(df.shape[0], channel_name))\n",
    "    for i in range(df.shape[0]):\n",
    "        target_fname = df.iloc[i]['image_file_name']\n",
    "#         if verbose:\n",
    "#             print(target_fname)\n",
    "        src = \"{}/{}\".format(IMAGES_DIR, target_fname) #f\"{IMAGES_DIR}/{target_fname}\"\n",
    "        dst = \"{}/{}/{}\".format(output_path,channel_name,target_fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--classes\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--input-data\", type=str, default=\"classes.txt\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    c_list = args.classes.split(',')\n",
    "    input_data = args.input_data\n",
    "        \n",
    "    CLASSES_FILE = os.path.join(input_path, input_data)\n",
    "\n",
    "    CLASS_COLS      = ['class_number','class_id']\n",
    "    \n",
    "    if len(c_list)==0:\n",
    "        # Otherwise, you can use the full set of species\n",
    "        CLASSES = []\n",
    "        for c in range(200):\n",
    "            CLASSES += [c + 1]\n",
    "        prefix = prefix + '-full'\n",
    "    else:\n",
    "        CLASSES = list(map(int, c_list))\n",
    "\n",
    "            \n",
    "    classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "\n",
    "    criteria = classes_df['class_number'].isin(CLASSES)\n",
    "    classes_df = classes_df[criteria]\n",
    "\n",
    "    class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "    print(class_name_list)\n",
    "    \n",
    "    \n",
    "    train_df, val_df, test_df = get_train_val_dataframes(input_path, CLASSES, SPLIT_RATIOS)\n",
    "        \n",
    "    for c in class_name_list:\n",
    "        os.mkdir('{}/{}/{}'.format(output_path, 'valid', c))\n",
    "        os.mkdir('{}/{}/{}'.format(output_path, 'test', c))\n",
    "        os.mkdir('{}/{}/{}'.format(output_path, 'train', c))\n",
    "\n",
    "    copy_files_for_channel(val_df,   'valid')\n",
    "    copy_files_for_channel(test_df,  'test')\n",
    "    copy_files_for_channel(train_df, 'train')\n",
    "    \n",
    "    # export manifest file for validation\n",
    "    train_m_file = \"{}/manifest/train.csv\".format(output_path)\n",
    "    train_df.to_csv(train_m_file, index=False)\n",
    "    test_m_file = \"{}/manifest/test.csv\".format(output_path)\n",
    "    test_df.to_csv(test_m_file, index=False)\n",
    "    val_m_file = \"{}/manifest/valid.csv\".format(output_path)\n",
    "    val_df.to_csv(val_m_file, index=False)\n",
    "    \n",
    "    print(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    ")\n",
    "import time \n",
    "\n",
    "timpstamp = str(time.time()).split('.')[0]\n",
    "# SKlearnProcessor for preprocessing\n",
    "output_prefix = f'{base_job_prefix}/outputs/{timpstamp}'\n",
    "output_s3_uri = f's3://{default_bucket}/{output_prefix}'\n",
    "\n",
    "class_selection = '13, 17, 35, 36, 47, 68, 73, 87'\n",
    "input_annotation = 'classes.txt'\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "processing_instance_count = 1\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(base_job_name = f\"{base_job_prefix}-preprocess\",  # choose any name\n",
    "                                    framework_version='0.20.0',\n",
    "                                    role=role,\n",
    "                                    instance_type=processing_instance_type,\n",
    "                                    instance_count=processing_instance_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    arguments=[\"--classes\", class_selection, \n",
    "               \"--input-data\", input_annotation],\n",
    "    inputs=[ProcessingInput(source=s3_raw_data, \n",
    "            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/train\", destination = output_s3_uri +'/train'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/valid\", destination = output_s3_uri +'/valid'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/test\", destination = output_s3_uri +'/test'),\n",
    "            ProcessingOutput(source=\"/opt/ml/processing/output/manifest\", destination = output_s3_uri +'/manifest'),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Outputs\n",
    "\n",
    "At the end of the lab, you dataset will be randomly split into train, valid, and test folders. YUou will also have a csv manifest file for each channel. Validate your results with the script below. **If you plan to complete other modules in this workshop, please keep these data.  Otherwise, you can clean up after this lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "response = s3_client.list_objects_v2(Bucket=default_bucket, Prefix=output_prefix)\n",
    "files = response.get(\"Contents\")\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(f\"file_name: {file['Key']}, size: {file['Size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
