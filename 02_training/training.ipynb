{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Tuning\n",
    "\n",
    "In this notebook we will train the model on the data we prepared in [Module 1: Preprocessing](../01_preprocessing/data_preprocessing.ipynb) using the AWS-managed Tensorflow container and a script describing the model used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and initialize parameters for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "boto3_session = boto3.Session()\n",
    "sagemaker_client = boto3_session.client(\"sagemaker\")\n",
    "\n",
    "account = sagemaker_session.account_id()\n",
    "region = sagemaker_session.boto_region_name\n",
    "default_bucket = sagemaker_session.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'cv-sagemaker-immersionday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_s3_uri = f's3://{default_bucket}/{prefix}/outputs'\n",
    "model_output_path = f's3://{default_bucket}/{prefix}/outputs/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning\n",
    "\n",
    "[Amazon SageMaker automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html), also known as hyperparameter optimization (HPO), finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure HPO Job\n",
    "Next, the tuning job with the following configurations need to be specified:\n",
    "- hyperparameters that SageMaker Automatic Model Tuning will tune: `dropout`, `batch-size`;\n",
    "- maximum number of training jobs it will run to optimize the objective metric: `6`\n",
    "- number of parallel training jobs that will run in the tuning job: `2`\n",
    "- the objective metric that Automatic Model Tuning will use is the accuracy of the validation data: `val_acc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'loss',      'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'acc',       'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'val_loss',  'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'val_acc',   'Regex': 'val_accuracy: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "TF_FRAMEWORK_VERSION = '2.4.1'\n",
    "DISTRIBUTION = {'parameter_server': {'enabled': False}}\n",
    "DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    \n",
    "training_instance_type = 'ml.c5.4xlarge'\n",
    "training_instance_count = 1\n",
    "shared_hyperparameters = { \"initial_epochs\": 5, 'fine_tuning_epochs': 20, 'data_dir': '/opt/ml/input/data' }\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"train-mobilenet.py\",\n",
    "    source_dir=\"code\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,   \n",
    "    hyperparameters=shared_hyperparameters,    \n",
    "    metric_definitions=metric_definitions,     \n",
    "    role=role,\n",
    "    framework_version=TF_FRAMEWORK_VERSION, \n",
    "    py_version='py37',     \n",
    "    base_job_name=prefix,\n",
    "    script_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"dropout\": ContinuousParameter(0.5, 0.8),\n",
    "    \"batch-size\": CategoricalParameter([8 , 16, 32, 64, 128, 256])\n",
    "}\n",
    "\n",
    "objective_metric_name = \"val_acc\"\n",
    "\n",
    "train_in = TrainingInput(s3_data=processed_data_s3_uri +'/train', distribution=DISTRIBUTION_MODE)\n",
    "val_in   = TrainingInput(s3_data=processed_data_s3_uri +'/valid', distribution=DISTRIBUTION_MODE)\n",
    "test_in  = TrainingInput(s3_data=processed_data_s3_uri +'/test', distribution=DISTRIBUTION_MODE)\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=2, # Low because of demo purposes\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name=\"cv-hpo\",\n",
    ")\n",
    "\n",
    "tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the experiment associated to the HPO job\n",
    "\n",
    "[SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) helps you organize, track, compare and evaluate machine learning (ML) experiments and model versions. SInce ML is a highly iterative process, Experiment helps data scientists and ML engineers to explore thousands of different models in an organized manner.  Exspecially when you are using tools like [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) and [Amazon SageMaker Autopilot](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html), it will help you explore a large number of combinations automatically, and quickly zoom in on high-performance models.\n",
    "\n",
    "The tuning job has an experiment automatically associated with it and we can explore the results in the SageMaker user interface. \n",
    "\n",
    "![hpo-experiment](statics/hpo-experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access the best model programatically as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri = tuner.best_estimator().latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(f\"\\nBest model artifact file is uploaded here: {best_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the best model into the prefix used for this project. Note that's possible to directly reference the tuner's output, as above, but given the modularity of this workshop, the artefacts are copied to a known location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {best_model_uri} {model_output_path}/model.tar.gz"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
